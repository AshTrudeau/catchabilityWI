---
title: "integrated population estimate and catch equation"
output: html_document
date: "2024-05-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, here, lubridate, lme4, loo, rstan, shinystan, truncnorm, MFEUtilities, RColorBrewer, cowplot)
```

Make 'fake' data. (catch is simulated, population densities are real)

```{r}
set.seed(34)

catch.fun.err<-function(effort, q_mu, q_d, q_a, q_l, popDensity, beta, error){
  catch=(effort*(q_mu+q_d+q_a+q_l)*popDensity^beta)*error
  return(catch)
}
# method of moments for getting lognormal distribution to draw from
mom.lognorm.mu<-function(mean, var){
  log(mean/sqrt((var/mean^2)+1))
}
mom.lognorm.sigma<-function(mean, var){
  sqrt(log((var/mean^2)+1))
}

# draw 'observed' population densities from distribution of draws for simulation of catch

popDensityObs<-read_csv(here::here("output","bayesian.pop.density.csv"))
popDensityObs$...1<-NULL

popDensity.param<-popDensityObs%>%
  mutate(var=sd.popDensity^2,
         mu=mom.lognorm.mu(mean.popDensity, var),
         sigma=mom.lognorm.sigma(mean.popDensity, var),
         popDensityDraw=rlnorm(nrow(popDensityObs), mu, sigma))

# output from population estimates needed for simulating data  
pe.data<-read_csv(here::here("output","recap.sumctmt.sumrt.surfaceArea.csv"))

```

and the rest of the simulated data

```{r}
# 90 unique dates
D<-90
# 20 unique anglers
A<-20
# 10 unique lakes
L<-13

q_d<-rlnorm(D, -1, 0.5)
q_a<-rlnorm(A, -2, 0.2)
q_l<-rlnorm(L, -2, 0.01)

q_mu<-0.2

popDensity<-popDensity.param$popDensityDraw
beta<-0.25

# D, A, and L indexes and their q values
dates<-data.frame(D=seq(1:D), q_d=q_d)
anglers<-data.frame(A=seq(1:A), q_a=q_a)
lakes<-data.frame(L=seq(1:L), q_l=q_l, sumCtMt=pe.data$sumCtMt, sumRt=pe.data$sumRt, surfaceArea=pe.data$surfaceArea, popDensity=popDensity)

# sample randomly with replacement from anglers and lakes for each date. then left_join approprate q values by A, D, and L selection

lakeSamples<-sample(lakes$L, 180, replace=TRUE)
anglerSamples<-sample(anglers$A, 540, replace=TRUE)


# 2 lakes visited each day, three anglers at each lake
fake.df.hier<-data.frame(D=rep(dates$D, each=6),
                         L=rep(lakeSamples, each=3),
                         A=anglerSamples)%>%
  left_join(dates, by="D")%>%
  left_join(anglers, by="A")%>%
  left_join(lakes, by="L")%>%
  mutate(beta=rep(0.25),
         q_mu=rep(0.1),
         effort=rlnorm(540, meanlog=1.25, sdlog=0.25),
         error=rlnorm(540, meanlog=0, sdlog=0.3),
         popDensity=popDensity,
         lmbCatch=floor(catch.fun.err(effort, q_mu, q_d, q_a, q_l, popDensity, beta, error)))

# that's a lot of it, but I still need sumCtMt, surfaceArea, and sumRt



fake.data.hier<-list(N=nrow(fake.df.hier),
                     D=D,
                     L=L,
                     A=A,
                     DD=fake.df.hier$D,
                     LL=fake.df.hier$L,
                     AA=fake.df.hier$A,
                     log_effort=log(fake.df.hier$effort),
                     lmbCatch=fake.df.hier$lmbCatch,
                    # surfaceArea=lakes$surfaceArea,
                    # sumCtMt=lakes$sumCtMt,
                    # sumRt=lakes$sumRt
                    surfaceArea=fake.df.hier$surfaceArea,
                    sumCtMt=fake.df.hier$sumCtMt,
                    sumRt=fake.df.hier$sumRt)

fake_fit_hier<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=fake.data.hier,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=2000,
                 cores=4,
                 refresh=0)
launch_shinystan(fake_fit_hier)
```

Time to fit to real data! This is a good place to clear the environment

Pull raw data from MFE database
```{r}
rm(list=ls())

# raw catch data
wd<-getwd()
db.dir<-paste0(wd, "/MFEdb/")
db<-"MFEdb_20220405.db"

dbTableList(db.dir, db)

lakes<-dbTable("lakes", fpath=db.dir, dbname=db)

# lake data

lakes.key<-lakes%>%
  dplyr::select(lakeID, lakeName, lat, long, WBIC, surfaceArea)
# Found lake missing surface area--135.97 ha
lakes.key[lakes.key$lakeID=="FD","surfaceArea"]<-135.97


projects<-dbTable("projects", fpath=db.dir, dbname=db)

fishSamples<-dbTable("fish_samples", fpath=db.dir, dbname=db)%>%
  filter(projectID%in%c("37","38") & gear=="AN")

fishInfo<-dbTable("fish_info", fpath=db.dir, dbname=db)%>%
  filter(sampleID%in%fishSamples$sampleID)%>%
  filter(str_length(caughtBy)<4)%>%
  mutate(caughtBy=str_trim(caughtBy))%>%
  filter(otu=="largemouth_bass")%>%
  dplyr::select(projectID:caughtBy, comments)%>%
  # join fishing effort
  left_join(fishSamples[,c("lakeID","sampleID","dayOfYear","dateSample","dateTimeSample","crew","effort","effortUnits","nAnglers")], by="sampleID")

# catch rates for each angler trip, correcting for some ambiguous initials first

ALK.lake.date<-c("LV_20190608",
                 "SM_20190615",
                 "WN_20190617",
                 "BY_20190622")

AMK.lake.date<-c("DS_20190613",
                 "BOT_20190615",
                 "SM_20190621",
                 "BOT_20190624",
                 "NH_20190713")

long.crew<-fishSamples%>%
  group_by(sampleID)%>%
  summarize(crew=unique(crew),
            effort=unique(effort))%>%
  # split crew into columns and then pivot longer
  separate("crew", paste("angler", 1:3, sep="_"), sep=", ", extra="drop")%>%
  pivot_longer(cols=angler_1:angler_3, names_to="angler_num", values_to="caughtBy", values_drop_na=T)%>%
  mutate(caughtBy=ifelse(caughtBy=="CMI","CI", caughtBy))%>%
  mutate(date=str_split_fixed(sampleID, "_", 4)[,3],
         lakeID=str_split_fixed(sampleID, "_", 2)[,1],
         lakeID_date=paste(lakeID, date, sep="_"))%>%
  mutate(caughtBy=ifelse(lakeID_date%in%AMK.lake.date, "AMK",
                         ifelse(lakeID_date%in%ALK.lake.date, "ALK", caughtBy)))
  
 #long.crew$date<-NULL
 #long.crew$lakeID<-NULL
 #long.crew$lakeID_date<-NULL
 
 
 all.data<-fishInfo%>%
  mutate(year=year(dateSample))%>%
  left_join(lakes.key, by="lakeID")
  

  
# now get catch rates from angling data and left join to long.crew. empty values can then be replaced with 0

cpue<-all.data%>%
  group_by(sampleID, caughtBy)%>%
  summarize(nCaught=n())



full.data<-long.crew%>%
  left_join(cpue, by=c("sampleID", "caughtBy"))%>%
  mutate(nCaught=ifelse(is.na(nCaught), 0, nCaught))



# join on mark recap data (sumCtRt and sumRt) and surface area

```


code for processing mark recap data

```{r}
fish_samples<-dbTable("fish_samples", fpath=db.dir)%>%
  filter(projectID%in%c("37"))%>%
  filter(useSampleMarkRecap=="yes")

# ok, we want to do projects 37 and 38 separately. 37 used AF (anal fin) tags, 38 used PIT tags
fish_info<-dbTable("fish_info", fpath=db.dir)%>%
  filter(projectID%in%c("37"))%>%
  filter(sampleID%in%fish_samples$sampleID)%>%
  filter(otu=="largemouth_bass")

fish_data<-inner_join(fish_samples, fish_info, by="sampleID")%>%
  mutate(lakeID=str_split_fixed(sampleID, "_", 2)[,1],
         clipRecapture=as.numeric(clipRecapture),
         clipRecapture=ifelse(is.na(clipRecapture),0,clipRecapture),
         tagged=ifelse(clipApply=="AF", 1, 0))

samples<-unique(fish_data$sampleID)

lakes<-dbTable("lakes", fpath=db.dir)

fish_samples<-data.frame(sampleID=unique(fish_data$sampleID))%>%
  mutate(method=str_split_fixed(sampleID, "_", 6)[,5],
         sampleDate=str_split_fixed(sampleID, "_", 4)[,3],
         sampleTime=str_split_fixed(sampleID, "_", 5)[,4],
         date_time=ymd_hm(paste(sampleDate, sampleTime, sep="_")),
         # adjust sampleDates for night electrofishing--if method==BE and sampleTime is in the evening before midnight, add one day to date.
         adjust=ifelse(method=="BE" & sampleTime<2359 & sampleTime>1200, 1, 0),
         adj_sampleDate=as.character(ymd(sampleDate)+days(1)),
         batchDate=ifelse(adjust==1, adj_sampleDate, as.character(ymd(sampleDate))),
         batchDate_method=paste(batchDate, method, sep="_"))

fish_pe<-left_join(fish_data, fish_samples[,c("sampleID","batchDate_method")], by="sampleID")%>%
  group_by(lakeID, batchDate_method)%>%
  summarize(markedNow=sum(tagged, na.rm=T),
            recapturedNow=sum(clipRecapture))%>%
  mutate(marked_cum=cumsum(markedNow),
         markedPrior=lag(marked_cum),
         markedPrior=ifelse(is.na(markedPrior), 0, markedPrior),
         allFishCaught=markedNow+recapturedNow)%>%
  ungroup()

# let's filter to lakes that had at least 1 recapture

recap_count<-fish_pe%>%
  group_by(lakeID)%>%
  summarize(nRecap=sum(recapturedNow))%>%
  arrange(desc(nRecap))%>%
  filter(nRecap>0)

recap_over_1<-filter(recap_count, nRecap>1)

# surface area for Found lake was missing. Got it from WI DNR (find a lake tool) and converted to hectares
lakes[lakes$lakeID=="FD",]$surfaceArea<-136.0

fish_pe_recap<-fish_pe%>%
  filter(lakeID%in%recap_count$lakeID)%>%
  mutate(CtMt=allFishCaught*markedPrior)%>%
  group_by(lakeID)%>%
  summarize(sumCtMt=sum(CtMt),
            sumRt=sum(recapturedNow))%>%
  ungroup()%>%
  left_join(lakes[,c("lakeID","surfaceArea")], by="lakeID")

```
Now join those datasets and index the unique lakeIDs and caughtBy initials

```{r}
data.join<-full.data%>%
  filter(lakeID%in%fish_pe_recap$lakeID)%>%
  left_join(fish_pe_recap, by="lakeID")


lakeID<-data.frame(lakeID=fish_pe_recap$lakeID,
                   L=seq(1:13),
                   sumCtMt=fish_pe_recap$sumCtMt,
                   sumRt=fish_pe_recap$sumRt,
                   surfaceArea=fish_pe_recap$surfaceArea)

anglerID<-data.frame(caughtBy=unique(data.join$caughtBy),
                     A=seq(1:18))

dateID<-data.frame(date=unique(data.join$date),
                   D=seq(1:42))

data.indexed<-data.join%>%
  left_join(dateID, by="date")%>%
  left_join(anglerID, by="caughtBy")%>%
  left_join(lakeID, by="lakeID")%>%
  mutate(log_effort=log(effort))%>%
  rename("lmbCatch"=nCaught)

data.list<-list(N=nrow(data.indexed),
                A=max(data.indexed$A),
                D=max(data.indexed$D),
                L=max(data.indexed$L),
                AA=data.indexed$A,
                DD=data.indexed$D,
                LL=data.indexed$L,
                lmbCatch=data.indexed$lmbCatch,
                log_effort=data.indexed$log_effort,
                sumCtMt=lakeID$sumCtMt,
                sumRt=lakeID$sumRt,
                surfaceArea=lakeID$surfaceArea)


real_fit<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)
launch_shinystan(real_fit)


# troubleshooting divergent transitions problem
# first, is it somehow caused by all the generated quantities I added?
# partially. But adding back log_mu_q_a/d/l fixed it entirely. Now r2_fixed is messed up (5/30)
test_fit<-stan(file="test_version.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)
```
Yes, putting back the log_mu_q_a/d/l stopped the divergent transitions for some reason. but now the r2 and vpc estimates are all messed up
Let's start looking at the draws to start interpreting parameter values (and making pretty graphs)

```{r}
print(real_fit, pars=c("log_q_mu","beta","phi","sigma_q_a", "sigma_q_d",  "sigma_q_l", "popDensity"), probs=c(.025,.5,.975))

```

```{r}

# draws is a list of draws from the posterior for each parameter
popDensity_fun<-function(draws){
  
  popDensity<-data.frame(draws$popDensity)
  names(popDensity)<-lakeID$lakeID

  popDensityLong<-popDensity%>%
    pivot_longer(cols=everything(), names_to="lakeID", values_to="popDensity")

  palette<-c(brewer.pal(12, "Paired"), "gray")

  ggplot(popDensityLong)+
    geom_density(aes(x=popDensity, fill=lakeID))+
    scale_fill_manual(values=palette)+
    facet_wrap(.~lakeID, nrow=3, scales="free_x")+
    theme_bw()
  
}

log_q_a_fun<-function(draws){
  q_a<-data.frame(draws$q_a)
  log_q_a<-data.frame(draws$log_q_a)
  names(log_q_a)<-anglerID$caughtBy

  log_q_aLong<-log_q_a%>%
    pivot_longer(cols=everything(), names_to="anglerID", values_to="log_q_a")

  palette<-c("#78b98f", "#de0ca3", "#74ee65", "#69306e", "#59a20c", "#903be2", "#c0e15c",
             "#891c1a", "#64d4fd", "#0a4f4e", "#e7ad79", "#1945c5", "#edb1ff", "#683d0d", "#fe707d",
             "#445a06", "#4787c9", "#fbd127")

  ggplot(log_q_aLong)+
    geom_density(aes(x=log_q_a, fill=anglerID))+
    geom_vline(xintercept=0, linetype="dashed")+
    scale_fill_manual(values=palette)+
    facet_wrap(.~anglerID, nrow=4)+
    theme_bw()

}

log_q_l_fun<-function(draws){
  log_q_l<-data.frame(draws$log_q_l)
  names(log_q_l)<-lakeID$lakeID

  log_q_lLong<-log_q_l%>%
  pivot_longer(cols=everything(), names_to="lakeID", values_to="log_q_l")


ggplot(log_q_lLong)+
    geom_density(aes(x=log_q_l, fill=lakeID))+
    geom_vline(xintercept=0, linetype="dashed")+
 # scale_fill_manual(values=palette)+
    facet_wrap(.~lakeID, nrow=4)+
    theme_bw()
}

log_q_d_fun<-function(draws){
  log_q_d<-data.frame(draws$log_q_d)
  names(log_q_d)<-dateID$date

  log_q_dLong<-log_q_d%>%
    pivot_longer(cols=everything(), names_to="date", values_to="log_q_d")%>%
    mutate(date=ymd(date))%>%
    group_by(date)%>%
    summarize(mean_log_q_d=mean(log_q_d),
            sd_log_q_d=sd(log_q_d),
            upper_95=mean_log_q_d+1.96*sd_log_q_d,
            lower_95=mean_log_q_d-1.96*sd_log_q_d)%>%
    ungroup()%>%
   mutate(year=year(date))


  ggplot(log_q_dLong)+
    geom_point(aes(x=date, y=mean_log_q_d))+
    geom_errorbar(aes(x=date, ymin=lower_95, ymax=upper_95))+
    facet_wrap(.~year, nrow=2, scales="free_x")+
    theme_bw()

}

other_params_fun<-function(draws){
  phi<-data.frame(draws$phi)
  names(phi)<-"phi"
  beta<-data.frame(draws$beta)
  names(beta)<-"beta"
  log_q_mu<-data.frame(draws$log_q_mu)
  names(log_q_mu)<-"log_q_mu"

  betaPlot<-ggplot(beta)+
    geom_density(aes(x=beta), fill="lightblue")+
    geom_vline(xintercept=0, linetype="dashed")+
    theme_bw()

  phiPlot<-ggplot(phi)+
    geom_density(aes(x=phi), fill="lightblue")+
    geom_vline(xintercept=0, linetype="dashed")+
    theme_bw()
  
  muPlot<-ggplot(log_q_mu)+
    geom_density(aes(x=log_q_mu), fill="lightblue")+
    geom_vline(xintercept=0, linetype="dashed")+
    theme_bw()

  
  plot_grid(betaPlot, phiPlot, muPlot)

}


```

http://vrl.cs.brown.edu/color for color palettes


These are the draws from the posterior from the"original" model before I start messing with priors
```{r}
drawsOriginal<-extract(real_fit)

popDensity_fun(draws=drawsOriginal)
log_q_a_fun(draws=drawsOriginal)
log_q_l_fun(draws=drawsOriginal)
log_q_d_fun(draws=drawsOriginal)
other_params_fun(draws=drawsOriginal)

```
look at q_a values for comparison
```{r}
draws<-extract(real_fit)
  q_a<-data.frame(draws$q_a)
  names(q_a)<-anglerID$caughtBy

  q_aLong<-q_a%>%
    pivot_longer(cols=everything(), names_to="anglerID", values_to="q_a")

  palette<-c("#78b98f", "#de0ca3", "#74ee65", "#69306e", "#59a20c", "#903be2", "#c0e15c",
             "#891c1a", "#64d4fd", "#0a4f4e", "#e7ad79", "#1945c5", "#edb1ff", "#683d0d", "#fe707d",
             "#445a06", "#4787c9", "#fbd127")

  ggplot(q_aLong)+
    geom_histogram(aes(x=q_a, fill=anglerID))+
    geom_vline(xintercept=0, linetype="dashed")+
    scale_fill_manual(values=palette)+
    facet_wrap(.~anglerID, nrow=4, scales="free_x")+
    theme_bw()

```


Next I'm going to get rid of the prior on PE, which I think is redundant


```{r}
real_fit_pe_prior<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

```


Test priors on population density
```{r}
draws<-extract(real_fit_pe_prior)
popDensity_fun(draws=draws)
log_q_a_fun(draws=draws)
log_q_l_fun(draws=draws)
log_q_d_fun(draws=draws)
other_params_fun(draws=draws)


```
The population densities become way more uncertain and on average, larger. The rest of the posteriors don't change much, though CIs get wider

What if I make priors on PE wider? (change SD to 3) also changing priors on population density to lognorm(0,2)
```{r}
real_fit_wide_pe_popDensity<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_widePEPop<-extract(real_fit_pe_prior)
popDensity_fun(draws=draws_widePEPop)
log_q_a_fun(draws=draws_widePEPop)
log_q_l_fun(draws=draws_widePEPop)
log_q_d_fun(draws=draws_widePEPop)
other_params_fun(draws=draws_widePEPop)


```
Priors on PE and popDensity have a strong effect on some lake-specific estimates, but not much effect on other parameters. I'm going to keep those priors wider for now. 

Test priors on phi and beta
beta first--changin sigma from 1 to 2 (wider prior)
```{r}
real_fit_wide_beta<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_wide_beta<-extract(real_fit_wide_beta)
popDensity_fun(draws=draws_wide_beta)
log_q_a_fun(draws=draws_wide_beta)
log_q_l_fun(draws=draws_wide_beta)
log_q_d_fun(draws=draws_wide_beta)
other_params_fun(draws=draws_wide_beta)

```
wide prior on beta slightly increases beta CI, butfew other noticeable changees

change mean on beta distribution from -1 to 0 (slightly wider)

```{r}
real_fit_mean_beta<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_mean_beta<-extract(real_fit_mean_beta)
popDensity_fun(draws=draws_mean_beta)
log_q_a_fun(draws=draws_mean_beta)
log_q_l_fun(draws=draws_mean_beta)
log_q_d_fun(draws=draws_mean_beta)
other_params_fun(draws=draws_mean_beta)

```
No obvious changes. Beta distribution is a little wider--right tail goes out to 2. I'm going to stick with -1 mean because of prior knowledge on beta values (hyperstability)

```{r}
real_fit_wide_phi<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_wide_phi<-extract(real_fit_wide_phi)
popDensity_fun(draws=draws_wide_phi)
log_q_a_fun(draws=draws_wide_phi)
log_q_l_fun(draws=draws_wide_phi)
log_q_d_fun(draws=draws_wide_phi)
other_params_fun(draws=draws_wide_phi)

```
Test priors on sigma_q_a,d, l.Exponential(1) instead of exponential(5), wider prior

```{r}
real_fit_wide_sigma<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_wide_sigma<-extract(real_fit_wide_sigma)
popDensity_fun(draws=draws_wide_sigma)
log_q_a_fun(draws=draws_wide_sigma)
log_q_l_fun(draws=draws_wide_sigma)
log_q_d_fun(draws=draws_wide_sigma)
other_params_fun(draws=draws_wide_sigma)

```
No obvious changes, except that log_q_mu parameter gets narrower

Test priors on log_q_mu
 (sigma from 1 to 2)
```{r}
real_fit_wide_log_q_mu<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_wide_log_q_mu<-extract(real_fit_wide_log_q_mu)
popDensity_fun(draws=draws_wide_log_q_mu)
log_q_a_fun(draws=draws_wide_log_q_mu)
log_q_l_fun(draws=draws_wide_log_q_mu)
log_q_d_fun(draws=draws_wide_log_q_mu)
other_params_fun(draws=draws_wide_log_q_mu)

```

There's some slight shrinkage towards the mean on q_a and q_l values
Test priors on mu_q_a,d, l

keep mean at 0, but increase sigma from 1 to 2
```{r}
real_fit_wide_mu_q<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_wide_mu_q<-extract(real_fit_wide_mu_q)
popDensity_fun(draws=draws_wide_mu_q)
log_q_a_fun(draws=draws_wide_mu_q)
log_q_l_fun(draws=draws_wide_mu_q)
log_q_d_fun(draws=draws_wide_mu_q)
other_params_fun(draws=draws_wide_mu_q)

```
Really noticeable shrinkage towards 0 (and wider CIs) on all q posteriors, including log_q_mu

What if I reduce sigma just a little bit, to 1.5? 

```{r}
real_fit_less_wide_mu_q<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_less_wide_mu_q<-extract(real_fit_less_wide_mu_q)
popDensity_fun(draws=draws_less_wide_mu_q)
log_q_a_fun(draws=draws_less_wide_mu_q)
log_q_l_fun(draws=draws_less_wide_mu_q)
log_q_d_fun(draws=draws_less_wide_mu_q)
other_params_fun(draws=draws_less_wide_mu_q)

```
Same, but less extreme

what if I make the prior more narrow?  0.5

```{r}
real_fit_narrow_mu_q<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_narrow_mu_q<-extract(real_fit_narrow_mu_q)
popDensity_fun(draws=draws_narrow_mu_q)
log_q_a_fun(draws=draws_narrow_mu_q)
log_q_l_fun(draws=draws_narrow_mu_q)
log_q_d_fun(draws=draws_narrow_mu_q)
other_params_fun(draws=draws_narrow_mu_q)

```
I see some slightdifferentiation in log_q_a with narrower prior, compare JC above (narrow prior) and below (original fit)
```{r}
log_q_a_fun(draws=drawsOriginal)

```

```{r}
summary<-summary(real_fit, R2=TRUE)
str(summary)
```


Check fit to data--posterior predictive sampling--does predicted data resemble observed data?

useful faq for cross validation: https://users.aalto.fi/~ave/CV-FAQ.html 
Test results whenlakes with only 1 recap are removed

citable source: Vehtari et al 2017, Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC
this was very helpful guidance: https://mc-stan.org/loo/articles/loo2-with-rstan.html 

```{r}
real_fit_loglik_pred<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik<-extract_log_lik(real_fit_loglik_pred, merge_chains=FALSE)
r_eff<-relative_eff(exp(log_lik), cores=2)
loo_mod<-loo(log_lik, r_eff=r_eff, cores=2)

loo_mod

loo_compare(loo_mod, loo)
```
one "bad" k diagnostic value (0.5% of observations)
this model fits better than the null model, yay

```{r}
#print(real_fit_loglik_pred, pars=c("predictions"), probs=c(.025,.5,.975))
parm_pred<-extract(real_fit_loglik_pred)

predictions<-data.frame(parm_pred$predictions)
names(predictions)<-seq(1:205)

pred.long<-predictions%>%
  pivot_longer(cols=everything(),
               names_to="anglerID",
               values_to="predictedCatch")

hist(sample(pred.long$predictedCatch, 205))
hist(data.list$lmbCatch)
```
Bayesian R2

```{r}
r2<-data.frame(r2=parm_pred$bayes_r2)
hist(r2)

r2_short<-sample_n(r2, 1000)

ggplot(r2_short)+
  geom_histogram(aes(x=r2))+
  xlab("Bayesian R2")+
  theme_bw()
ggsave(here::here("figures","posteriors", "bayes_r2.png"), height=4, width=6)
```


Predictions are 8000 repetitions of predictions for 205 observations

```{r}
predictions.df<-data.frame(N=rep(seq(1:205), 8000),
                           pred=pred.long$predictedCatch,
                           AA=rep(data.list$AA), 8000,
                           DD=rep(data.list$DD), 8000,
                           LL=rep(data.list$LL), 8000,
                           obs=rep(data.list$lmbCatch), 8000)

fewer.obs<-sample_n(predictions.df,1000)

ggplot(fewer.obs)+
  geom_point(aes(x=obs, y=pred))+
  geom_abline(slope=1, intercept=0,linetype="dashed")+
  xlab("observed LMB catch")+
  ylab("predicted LMB catch")+
  theme_bw()
ggsave(here::here("figures","posteriors", "pred.vs.obs.png"), height=4, width=6)
```
That doesn't look too bad!One thing, though, is that the model doesn't predict zero catches, just very small numbers. If I round down, it still predicts fewer zeroes than observed

```{r}

ggplot(fewer.obs)+
geom_histogram(aes(x=obs), fill="gray", alpha=0.5)+
geom_histogram(aes(x=pred), fill="red", alpha=0.5)+
  theme_bw()
ggsave(here::here("figures","posteriors", "pred.obs.hist.png"), height=4, width=6)

```


saving posteriors

```{r}
popDensity_fun(draws=parm_pred)
ggsave(here::here("figures","posteriors","popDensity.png"))
log_q_a_fun(draws=parm_pred)
ggsave(here::here("figures", "posteriors", "log_q_a.png"))
log_q_l_fun(draws=parm_pred)
ggsave(here::here("figures","posteriors","log_q_l.png"))
log_q_d_fun(draws=parm_pred)
ggsave(here::here("figures","posteriors","log_q_d.png"))
other_params_fun(draws=parm_pred)
ggsave(here::here("figures","posteriors","beta.phi.log_q_mu.png"))

```
null model

```{r}
null.data.list<-list(N=205,
                    lmbCatch=data.indexed$lmbCatch)

null_fit<-stan(file="null.model.stan",
                 data=null.data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

extract<-extract(null_fit)

log_lik<-extract_log_lik(null_fit, merge_chains=FALSE)
r_eff<-relative_eff(exp(log_lik), cores=2)
loo<-loo(log_lik, r_eff=r_eff, cores=2)

loo
```

Now looking into ICC, partitioning variance among random effects

Forum post gets into it a little https://discourse.mc-stan.org/t/extending-the-interclass-correlation-coefficient-icc-aka-repeatability-aka-heritability-for-bayesian-models/27793 

Responses suggest 'Doors and Corners' pre-print, which I need to finish reading

If I can't cite a pre-print, it cites several peer-reviewed papers using this approach

Post-meeting 5/10: 

Trying student t distribution on log_mu_q parameters

```{r}
real_fit_student<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_student<-extract(real_fit_student)
popDensity_fun(draws=draws_student)
log_q_a_fun(draws=draws_student)
log_q_l_fun(draws=draws_student)
log_q_d_fun(draws=draws_student)
other_params_fun(draws=draws_student)


```
Yeah, output is about the same. Slightly different shape on log_q_l distributions. Variance is also wider on parameters


Reduced dataset

```{r}
data.join.red<-full.data%>%
  filter(lakeID%in%fish_pe_recap$lakeID)%>%
  left_join(fish_pe_recap, by="lakeID")%>%
  filter(lakeID%in%recap_over_1$lakeID)

fish_pe_recap_red<-filter(fish_pe_recap, lakeID%in%recap_over_1$lakeID)

lakeID<-data.frame(lakeID=fish_pe_recap_red$lakeID,
                   L=seq(1:10),
                   sumCtMt=fish_pe_recap_red$sumCtMt,
                   sumRt=fish_pe_recap_red$sumRt,
                   surfaceArea=fish_pe_recap_red$surfaceArea)

anglerID<-data.frame(caughtBy=unique(data.join.red$caughtBy),
                     A=seq(1:16))

dateID<-data.frame(date=unique(data.join.red$date),
                   D=seq(1:30))

data.indexed.red<-data.join.red%>%
  left_join(dateID, by="date")%>%
  left_join(anglerID, by="caughtBy")%>%
  left_join(lakeID, by="lakeID")%>%
  mutate(log_effort=log(effort))%>%
  rename("lmbCatch"=nCaught)

data.list.red<-list(N=nrow(data.indexed.red),
                A=max(data.indexed.red$A),
                D=max(data.indexed.red$D),
                L=max(data.indexed.red$L),
                AA=data.indexed.red$A,
                DD=data.indexed.red$D,
                LL=data.indexed.red$L,
                lmbCatch=data.indexed.red$lmbCatch,
                log_effort=data.indexed.red$log_effort,
                sumCtMt=lakeID$sumCtMt,
                sumRt=lakeID$sumRt,
                surfaceArea=lakeID$surfaceArea)

real_fit_reduced<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list.red,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

draws_reduced<-extract(real_fit_reduced)
popDensity_fun(draws=draws_reduced)
log_q_a_fun(draws=draws_reduced)
log_q_l_fun(draws=draws_reduced)
log_q_d_fun(draws=draws_reduced)
other_params_fun(draws=draws_reduced)

```

No obvious differences--the shape of the beta posterior changed a little, and there are many fewer 2019 observations. 