---
title: "Varying effects beta"
output: html_document
date: "2024-08-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, here, lubridate, lme4, loo, rstan, shinystan, truncnorm, MFEUtilities, RColorBrewer, cowplot, bayesplot, cmdstanr, posterior)
```

Pull raw data from MFE database
```{r}
rm(list=ls())

# raw catch data
wd<-getwd()
db.dir<-paste0(wd, "/MFEdb/")
db<-"MFEdb_20220405.db"

dbTableList(db.dir, db)

lakes<-dbTable("lakes", fpath=db.dir, dbname=db)
#crew<-dbTable("crew", fpath=db.dir, dbname=db)%>%
#  filter(year%in%c("2018","2019"))
#write.csv(crew, "crew.csv")

# lake data

lakes.key<-lakes%>%
  dplyr::select(lakeID, lakeName, lat, long, WBIC, surfaceArea)
# Found lake missing surface area--135.97 ha
lakes.key[lakes.key$lakeID=="FD","surfaceArea"]<-135.97


projects<-dbTable("projects", fpath=db.dir, dbname=db)

fishSamples<-dbTable("fish_samples", fpath=db.dir, dbname=db)%>%
  filter(projectID%in%c("37") & gear=="AN")%>%
    mutate(nAnglers=as.numeric(nAnglers),
    effort=effort/nAnglers)


fishInfo<-dbTable("fish_info", fpath=db.dir, dbname=db)%>%
  filter(sampleID%in%fishSamples$sampleID)%>%
  filter(str_length(caughtBy)<4)%>%
  mutate(caughtBy=str_trim(caughtBy))%>%
  filter(otu=="largemouth_bass")%>%
  dplyr::select(projectID:caughtBy, comments)%>%
  # join fishing effort
  left_join(fishSamples[,c("lakeID","sampleID","dayOfYear","dateSample","dateTimeSample","crew","effort","effortUnits","nAnglers")], by="sampleID")

# catch rates for each angler trip, correcting for some ambiguous initials first

ALK.lake.date<-c("LV_20190608",
                 "SM_20190615",
                 "WN_20190617",
                 "BY_20190622")

AMK.lake.date<-c("DS_20190613",
                 "BOT_20190615",
                 "SM_20190621",
                 "BOT_20190624",
                 "NH_20190713")

long.crew<-fishSamples%>%
  group_by(sampleID)%>%
  summarize(crew=unique(crew),
            effort=unique(effort))%>%
  # split crew into columns and then pivot longer
  separate("crew", paste("angler", 1:3, sep="_"), sep=", ", extra="drop")%>%
  pivot_longer(cols=angler_1:angler_3, names_to="angler_num", values_to="caughtBy", values_drop_na=T)%>%
  mutate(caughtBy=ifelse(caughtBy=="CMI","CI", caughtBy))%>%
  mutate(date=str_split_fixed(sampleID, "_", 4)[,3],
         lakeID=str_split_fixed(sampleID, "_", 2)[,1],
         lakeID_date=paste(lakeID, date, sep="_"))%>%
  mutate(caughtBy=ifelse(lakeID_date%in%AMK.lake.date, "AMK",
                         ifelse(lakeID_date%in%ALK.lake.date, "ALK", caughtBy)))
  
 #long.crew$date<-NULL
 #long.crew$lakeID<-NULL
 #long.crew$lakeID_date<-NULL
 
 
 all.data<-fishInfo%>%
  mutate(year=year(dateSample))%>%
  left_join(lakes.key, by="lakeID")
  

  
# now get catch rates from angling data and left join to long.crew. empty values can then be replaced with 0

cpue<-all.data%>%
  group_by(sampleID, caughtBy)%>%
  summarize(nCaught=n())



full.data<-long.crew%>%
  left_join(cpue, by=c("sampleID", "caughtBy"))%>%
  mutate(nCaught=ifelse(is.na(nCaught), 0, nCaught))



# join on mark recap data (sumCtRt and sumRt) and surface area

```


code for processing mark recap data

```{r}
fish_samples<-dbTable("fish_samples", fpath=db.dir)%>%
  filter(projectID%in%c("37"))%>%
  filter(useSampleMarkRecap=="yes")

# ok, we want to do projects 37 and 38 separately. 37 used AF (anal fin) tags, 38 used PIT tags
fish_info<-dbTable("fish_info", fpath=db.dir)%>%
  filter(projectID%in%c("37"))%>%
  filter(sampleID%in%fish_samples$sampleID)%>%
  filter(otu=="largemouth_bass")

fish_data<-inner_join(fish_samples, fish_info, by="sampleID")%>%
  mutate(lakeID=str_split_fixed(sampleID, "_", 2)[,1],
         clipRecapture=as.numeric(clipRecapture),
         clipRecapture=ifelse(is.na(clipRecapture),0,clipRecapture),
         tagged=ifelse(clipApply=="AF", 1, 0))

samples<-unique(fish_data$sampleID)

lakes<-dbTable("lakes", fpath=db.dir)

fish_samples<-data.frame(sampleID=unique(fish_data$sampleID))%>%
  mutate(method=str_split_fixed(sampleID, "_", 6)[,5],
         sampleDate=str_split_fixed(sampleID, "_", 4)[,3],
         sampleTime=str_split_fixed(sampleID, "_", 5)[,4],
         date_time=ymd_hm(paste(sampleDate, sampleTime, sep="_")),
         # adjust sampleDates for night electrofishing--if method==BE and sampleTime is in the evening before midnight, add one day to date.
         adjust=ifelse(method=="BE" & sampleTime<2359 & sampleTime>1200, 1, 0),
         adj_sampleDate=as.character(ymd(sampleDate)+days(1)),
         batchDate=ifelse(adjust==1, adj_sampleDate, as.character(ymd(sampleDate))),
         batchDate_method=paste(batchDate, method, sep="_"))

fish_pe<-left_join(fish_data, fish_samples[,c("sampleID","batchDate_method")], by="sampleID")%>%
  group_by(lakeID, batchDate_method)%>%
  summarize(markedNow=sum(tagged, na.rm=T),
            recapturedNow=sum(clipRecapture))%>%
  mutate(marked_cum=cumsum(markedNow),
         markedPrior=lag(marked_cum),
         markedPrior=ifelse(is.na(markedPrior), 0, markedPrior),
         allFishCaught=markedNow+recapturedNow)%>%
  ungroup()

# let's filter to lakes that had at least 1 recapture

recap_count<-fish_pe%>%
  group_by(lakeID)%>%
  summarize(nRecap=sum(recapturedNow))%>%
  arrange(desc(nRecap))%>%
  filter(nRecap>0)

recap_over_1<-filter(recap_count, nRecap>1)

# surface area for Found lake was missing. Got it from WI DNR (find a lake tool) and converted to hectares
lakes[lakes$lakeID=="FD",]$surfaceArea<-136.0

fish_pe_recap<-fish_pe%>%
  filter(lakeID%in%recap_count$lakeID)%>%
  mutate(CtMt=allFishCaught*markedPrior)%>%
  group_by(lakeID)%>%
  summarize(sumCtMt=sum(CtMt),
            sumRt=sum(recapturedNow))%>%
  ungroup()%>%
  left_join(lakes[,c("lakeID","surfaceArea")], by="lakeID")

```

```{r}
data.join<-full.data%>%
  filter(lakeID%in%fish_pe_recap$lakeID)%>%
  left_join(fish_pe_recap, by="lakeID")


lakeID<-data.frame(lakeID=fish_pe_recap$lakeID,
                   L=seq(1:13),
                   sumCtMt=fish_pe_recap$sumCtMt,
                   sumRt=fish_pe_recap$sumRt,
                   surfaceArea=fish_pe_recap$surfaceArea)

anglerID<-data.frame(caughtBy=unique(data.join$caughtBy),
                     A=seq(1:18))

dateID<-data.frame(date=unique(data.join$date),
                   D=seq(1:42))

data.indexed<-data.join%>%
  left_join(dateID, by="date")%>%
  left_join(anglerID, by="caughtBy")%>%
  left_join(lakeID, by="lakeID")%>%
  mutate(log_effort=log(effort))%>%
  rename("lmbCatch"=nCaught)


```




```{r}
data.list<-list(N=nrow(data.indexed),
                A=max(data.indexed$A),
                D=max(data.indexed$D),
                L=max(data.indexed$L),
                AA=data.indexed$A,
                DD=data.indexed$D,
                LL=data.indexed$L,
                lmbCatch=data.indexed$lmbCatch,
                log_effort=data.indexed$log_effort,
                sumCtMt=lakeID$sumCtMt,
                sumRt=lakeID$sumRt,
                surfaceArea=lakeID$surfaceArea
                )

# trying a run that generates posterior predictions from the random effects I already have (rather than generating new ones)

fit_vary_slope<-stan(file="varying.effects.beta.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

post_vary_slope<-extract(fit_vary_slope)
ppc_dens_overlay(data.list$lmbCatch, post_vary_slope$posterior_pred_check[c(1:100),])

log_lik_vary_slope<-extract_log_lik(fit_vary_slope, merge_chains=FALSE)
r_eff_vary_slope<-relative_eff(exp(log_lik_vary_slope))

loo_vary_slope<-loo(log_lik_vary_slope, r_eff=r_eff_vary_slope, cores=4)

print(loo_vary_slope)


#launch_shinystan(fit_vary_slope)
post<-extract(fit_vary_slope)

angler_effect<-post$angler_effect
mean_angler_effect<-colMeans(angler_effect)

plot(mean_angler_effect[,2]~mean_angler_effect[,1])

```
negative correlation--at a higher intercept, there tends to be a lower beta, or more hyperstable catch rates (which makes sense)



Can I plot the draws of slope and intercept by angler? 

```{r}
angler_intercept<-as.data.frame(angler_effect[,,1])
names(angler_intercept)<-c(paste("angler", c(1:18), sep="_"))

angler_int_long<-angler_intercept%>%
  pivot_longer(cols=everything(), names_to = "angler", values_to="intercept")


angler_slope<-as.data.frame(angler_effect[,,2])
names(angler_slope)<-paste0("angler_", seq(1:18))

angler_slope_long<-angler_slope%>%
  pivot_longer(cols=everything(), names_to="angler", values_to="slope")

sum(angler_int_long$angler!=angler_slope_long$angler)
```

```{r}
# join is very slow--unnecessary? probably can just bind together
angler_params<-cbind.data.frame(angler_int_long, angler_slope_long[,c("slope")])

ggplot(angler_params)+
  geom_density(aes(x=intercept))+
  geom_vline(xintercept=0, linetype="dashed")+
  facet_wrap(vars(angler))+
  theme_bw()

ggplot(angler_params)+
  geom_density(aes(x=slope))+
  geom_vline(xintercept=0, linetype="dashed")+
  facet_wrap(vars(angler))+
  theme_bw()

```
now sd for intercept and slope among anglers

```{r}
sd_angler_params<-as.data.frame(post$sigma_angler)
names(sd_angler_params)<-c("sd_intercept","sd_slope")




ggplot(sd_angler_params)+
  geom_density(aes(x=sd_intercept))+
  geom_vline(xintercept=0, linetype="dashed")+
  ggtitle("sd_intercept")+
  theme_bw()

ggplot(sd_angler_params)+
  geom_density(aes(x=sd_slope))+
  geom_vline(xintercept=0, linetype="dashed")+
  ggtitle("sd_slope")+
  theme_bw()


```




Function to pivot, and visualize for the rest of the parameters
```{r}
# give it a named df
plot_param<-function(posterior, param.name){
  param.df=as.data.frame(posterior[[param.name]])
  long=pivot_longer(param.df, cols=everything(), names_to=param.name)
  ggplot(long)+
    geom_density(aes(x=value))+
    #facet_wrap(var(param.name))+
    geom_vline(xintercept=0, linetype="dashed")+
    ggtitle(param.name)+
    theme_bw()
}

plot_param(post, "phi")
plot_param(post, "angler_global")
plot_param(post, "sigma_date")
plot_param(post, "sigma_lake")

```
population densities
```{r}
pop.density<-as.data.frame(post$popDensity)
names(pop.density)<-paste0("lake_", seq(1:13))

pop.density.long<-pop.density%>%
  pivot_longer(cols=everything(), names_to="lake", values_to="popDensity")

ggplot(pop.density.long)+
  geom_density(aes(x=popDensity))+
  facet_wrap(vars(lake), scales="free_x")+
  ylim(0,1)+
  geom_vline(xintercept=0, linetype="dashed")+
  theme_bw()

```

oof, those are pretty bad population density estimates


```{r}
pred_draw<-post$catch_pred[1,]
obs_catch<-data.indexed$lmbCatch

ggplot()+
  geom_point(aes(x=obs_catch, y=pred_draw))+
  geom_abline(slope=1, intercept=0)+
  theme_bw()

```

```{r}
ppc_dens_overlay(data.list$lmbCatch, post$catch_pred[c(1:100),])
```
compare to original fit

```{r}
fit_vanilla<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)
post_vanilla<-extract(fit_vanilla)
ppc_dens_overlay(data.list$lmbCatch, post_vanilla$posterior_pred_check[c(1:100),])

log_lik_vanilla<-extract_log_lik(fit_vanilla, merge_chains=FALSE)
r_eff_vanilla<-relative_eff(exp(log_lik_vanilla))

loo_vanilla<-loo(log_lik_vanilla, r_eff=r_eff_vanilla, cores=4)

print(loo_vanilla)
print(loo_vary_slope)

comp<-loo_compare(loo_vanilla, loo_vary_slope)
print(comp)
```
No important difference


Try generating new random effects? see how different it is
```{r}
fit_slope_zinf<-stan(file="vary.beta.by.angler.zinf.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

post_slope_zinf<-extract(fit_slope_zinf)
ppc_dens_overlay(data.list$lmbCatch, post_slope_zinf$catch_pred[c(1:100),])




```

add zinf?
lol no

Model comparison with Pareto-smoothed importance sampling LOO (PSIS-LOO)

Random slopes did not improve the model fit

Testing random effects

specifying initial values might help

```{r}
hier.inits<-function(){
  q_mu=rlnorm(1, -2, 0.5)
  beta=rlnorm(1, 0, 0.5)
  phi=runif(1, min=0, max=10000)
  mu_q_a=rnorm(1, -1,0.5)
  mu_q_d=rnorm(1, -1,0.5)
  mu_q_l=rnorm(1, -1,0.5)
  sigma_q_a=rexp(1, 7)
  sigma_q_d=rexp(1, 7)
  sigma_q_l=rexp(1, 7)
  init<-list(q_mu=q_mu,
             beta=beta,
             phi=phi,
             mu_q_a=mu_q_a,
             mu_q_d=mu_q_d,
             mu_q_l=mu_q_l,
             sigma_q_a=sigma_q_a,
             sigma_q_d=sigma_q_d,
             sigma_q_l=sigma_q_l)
  return(init)
}


```


```{r}
fit_full<-stan(file="noncentered.hierarchical.LINEARIZED.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_fit_full<-extract_log_lik(fit_full, merge_chains=FALSE)
r_eff_full<-relative_eff(exp(log_lik_fit_full))
loo_full<-loo(log_lik_fit_full, r_eff=r_eff_full, cores=4)


fit_mean_q<-stan(file="model_test_mean_q.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_mean_q<-extract_log_lik(fit_mean_q, merge_chains=FALSE)
r_eff_mean_q<-relative_eff(exp(log_lik_mean_q))
loo_mean_q<-loo(log_lik_mean_q, r_eff=r_eff_mean_q, cores=4)
# yikes that's real bad on the diagnostics

fit_angler_only<-stan(file="model_test_angler_only.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_angler_only<-extract_log_lik(fit_angler_only, merge_chains=FALSE)
r_eff_angler_only<-relative_eff(exp(log_lik_angler_only))
loo_angler_only<-loo(log_lik_angler_only, r_eff=r_eff_angler_only, cores=4)


fit_date_only<-stan(file="model_test_date_only.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_date_only<-extract_log_lik(fit_date_only, merge_chains=FALSE)
r_eff_date_only<-relative_eff(exp(log_lik_date_only))
loo_date_only<-loo(log_lik_date_only, r_eff=r_eff_date_only, cores=4)


fit_lake_only<-stan(file="model_test_lake_only.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_lake_only<-extract_log_lik(fit_lake_only, merge_chains=FALSE)
r_eff_lake_only<-relative_eff(exp(log_lik_lake_only))
loo_lake_only<-loo(log_lik_lake_only, r_eff=r_eff_lake_only, cores=4)


fit_angler_date<-stan(file="model_test_angler_date.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_angler_date<-extract_log_lik(fit_angler_date, merge_chains=FALSE)
r_eff_angler_date<-relative_eff(exp(log_lik_angler_date))
loo_angler_date<-loo(log_lik_angler_date, r_eff=r_eff_angler_date, cores=4)


fit_angler_lake<-stan(file="model_test_angler_lake.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_angler_lake<-extract_log_lik(fit_angler_lake, merge_chains=FALSE)
r_eff_angler_lake<-relative_eff(exp(log_lik_angler_lake))
loo_angler_lake<-loo(log_lik_angler_lake, r_eff=r_eff_angler_lake, cores=4)




fit_date_lake<-stan(file="model_test_date_lake.stan",
                 data=data.list,
                 #init=hier.inits,
                 control=list(stepsize=0.1, max_treedepth=15),
                 chains=4,
                 warmup=1000,
                 iter=3000,
                 cores=4,
                 refresh=0)

log_lik_date_lake<-extract_log_lik(fit_date_lake, merge_chains=FALSE)
r_eff_date_lake<-relative_eff(exp(log_lik_date_lake))
loo_date_lake<-loo(log_lik_date_lake, r_eff=r_eff_date_lake, cores=4)


comp<-loo_compare(loo_full, loo_angler_only, loo_date_only, loo_lake_only, loo_angler_date, loo_angler_lake, loo_date_lake, loo_vary_slope)
print(comp)


```

Yeah, the full model is the best fit, though a little close to the one with just angler and date. I'll keep lake in there with the goal of comparing its (lack of) an effect with the other two random effects. (or, lack of an effect outside of population density)

when the varying slope model is included, it is the best fit, but negligibly next to the intercepts-only model. 

```{r}
launch_shinystan(fit_full)
```

